{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47301771-2518-4d17-9edc-5a0b81d9d6c2",
   "metadata": {},
   "source": [
    "### Starting with Spark\n",
    "<img src=\"http://spark.apache.org/images/spark-logo.png\" height=100>\n",
    "\n",
    "* How To Install Java with Apt on Ubuntu:\n",
    "\n",
    "```\n",
    "$ sudo apt update\n",
    "$ sudo apt install default-jre\n",
    "$ sudo apt install openjdk-11-jre-headless\n",
    "$ sudo apt install openjdk-8-jre-headless\n",
    "\n",
    "* Installing Scala\n",
    "\n",
    "$ sudo apt install scala\n",
    "```\n",
    "* Download Spark from https://spark.apache.org/downloads.html, extract it and put it in the **`~`**\n",
    "\n",
    "```\n",
    "~$ tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz \n",
    "\n",
    "# rename it to spark\n",
    "~$ mv spark-3.2.0-bin-hadoop3.2 spark  \n",
    "```\n",
    "* add this part to `.zshrc` or `.bashrc`\n",
    "\n",
    "```\n",
    "#Spark path (based on your computer)\n",
    "export SPARK_HOME=\"$HOME/spark\"\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "```\n",
    "* Restart `.zshrc` or `.bashrc`\n",
    "\n",
    "```\n",
    "$ source ~/.bashrc   # Or source ~/.zshrc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c58f28-2801-421f-ae65-012840fd5318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/siniorone/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260910c5-362d-4b7c-bd3f-3bcb647fceac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae01e89-77ec-4983-be1c-84d16e5a3453",
   "metadata": {},
   "source": [
    "### Starting with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d120847-0b76-427f-a461-7eaa5d7f2416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/spark.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = sc.textFile(\"data/spark.txt\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e3314-eec8-4218-a953-75f86145720c",
   "metadata": {},
   "source": [
    "#### some RDD actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c90d3d-64c7-40e2-b873-d37df1ba519f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef6dfaf-7f6d-4645-9163-313e04514e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apache Spark has its architectural foundation in the resilient distributed dataset '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26fca4-6cc1-4762-a03b-8fb302944263",
   "metadata": {},
   "source": [
    "#### let’s try a transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb297b1f-f546-4d51-b22d-c87dafd03847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinesWithSpark = spark.filter(lambda line:'Spark' in line)\n",
    "LinesWithSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778192f7-ff62-4df5-a3b0-5db1f6237760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def36d4f-3ce4-4455-8568-30d08c25086d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apache Spark has its architectural foundation in the resilient distributed dataset '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinesWithSpark.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc10b4b-fb1f-4449-a431-799b3bfa8159",
   "metadata": {},
   "source": [
    "## More on RDD Operations\n",
    "find the line from that \"poem.txt\" file with the most words in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40024301-e625-46de-8d78-4195fb7b9dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a51f01-d648-4db3-821e-6acacd64b364",
   "metadata": {},
   "source": [
    "Or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dee1af72-a9b0-4f31-b360-d88e13445aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_(a, b):\n",
    "    if a > b:\n",
    "        return a\n",
    "    else:\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7804e5e1-405e-4ecd-b7b2-430b4c824110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.map(lambda line: len(line.split())).reduce(max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7c6b755-1d38-474c-adfb-45b9bc7b8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = spark.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "001528ce-bde1-4aa0-a42b-3d35a8753ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 6)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee41ae42-b9b5-4d4f-b9c2-7a53c75da96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 6),\n",
       " ('Spark', 12),\n",
       " ('has', 1),\n",
       " ('its', 2),\n",
       " ('architectural', 1),\n",
       " ('foundation', 1),\n",
       " ('in', 7),\n",
       " ('the', 23),\n",
       " ('resilient', 1),\n",
       " ('distributed', 8),\n",
       " ('dataset', 1),\n",
       " ('(RDD),', 1),\n",
       " ('a', 18),\n",
       " ('read-only', 1),\n",
       " ('multiset', 1),\n",
       " ('of', 12),\n",
       " ('data', 4),\n",
       " ('items', 1),\n",
       " ('over', 1),\n",
       " ('cluster', 5),\n",
       " ('machines,', 1),\n",
       " ('that', 2),\n",
       " ('is', 7),\n",
       " ('maintained', 1),\n",
       " ('fault-tolerant', 1),\n",
       " ('way.', 1),\n",
       " ('The', 3),\n",
       " ('Dataframe', 1),\n",
       " ('API', 3),\n",
       " ('was', 2),\n",
       " ('released', 1),\n",
       " ('as', 4),\n",
       " ('an', 1),\n",
       " ('abstraction', 1),\n",
       " ('on', 6),\n",
       " ('top', 1),\n",
       " ('RDD,', 1),\n",
       " ('followed', 1),\n",
       " ('by', 3),\n",
       " ('Dataset', 3),\n",
       " ('API.', 2),\n",
       " ('In', 1),\n",
       " ('1.x,', 1),\n",
       " ('RDD', 3),\n",
       " ('primary', 1),\n",
       " ('application', 1),\n",
       " ('programming', 1),\n",
       " ('interface', 2),\n",
       " ('(API),', 1),\n",
       " ('but', 1),\n",
       " ('2.x', 1),\n",
       " ('use', 2),\n",
       " ('encouraged', 1),\n",
       " ('even', 1),\n",
       " ('though', 1),\n",
       " ('not', 2),\n",
       " ('deprecated.', 1),\n",
       " ('technology', 1),\n",
       " ('still', 1),\n",
       " ('underlies', 1),\n",
       " ('and', 5),\n",
       " ('RDDs', 3),\n",
       " ('were', 1),\n",
       " ('developed', 1),\n",
       " ('2012', 1),\n",
       " ('response', 1),\n",
       " ('to', 3),\n",
       " ('limitations', 1),\n",
       " ('MapReduce', 3),\n",
       " ('computing', 1),\n",
       " ('paradigm,', 1),\n",
       " ('which', 3),\n",
       " ('forces', 1),\n",
       " ('particular', 1),\n",
       " ('linear', 1),\n",
       " ('dataflow', 1),\n",
       " ('structure', 1),\n",
       " ('programs:', 1),\n",
       " ('programs', 2),\n",
       " ('read', 1),\n",
       " ('input', 1),\n",
       " ('from', 1),\n",
       " ('disk,', 1),\n",
       " ('map', 1),\n",
       " ('function', 2),\n",
       " ('across', 1),\n",
       " ('data,', 1),\n",
       " ('reduce', 1),\n",
       " ('results', 2),\n",
       " ('map,', 1),\n",
       " ('store', 1),\n",
       " ('reduction', 1),\n",
       " ('disk.', 1),\n",
       " (\"Spark's\", 1),\n",
       " ('working', 1),\n",
       " ('set', 2),\n",
       " ('for', 5),\n",
       " ('offers', 1),\n",
       " ('(deliberately)', 1),\n",
       " ('restricted', 1),\n",
       " ('form', 1),\n",
       " ('shared', 1),\n",
       " ('memory.', 1),\n",
       " ('Inside', 1),\n",
       " ('workflow', 1),\n",
       " ('managed', 1),\n",
       " ('directed', 1),\n",
       " ('acyclic', 1),\n",
       " ('graph', 1),\n",
       " ('(DAG).', 1),\n",
       " ('Nodes', 1),\n",
       " ('represent', 2),\n",
       " ('while', 1),\n",
       " ('edges', 1),\n",
       " ('operations', 1),\n",
       " ('RDDs.', 1),\n",
       " ('facilitates', 1),\n",
       " ('implementation', 1),\n",
       " ('both', 1),\n",
       " ('iterative', 2),\n",
       " ('algorithms,', 1),\n",
       " ('visit', 1),\n",
       " ('their', 1),\n",
       " ('multiple', 1),\n",
       " ('times', 1),\n",
       " ('loop,', 1),\n",
       " ('interactive/exploratory', 1),\n",
       " ('analysis,', 1),\n",
       " ('i.e.,', 1),\n",
       " ('repeated', 1),\n",
       " ('database-style', 1),\n",
       " ('querying', 1),\n",
       " ('data.', 1),\n",
       " ('latency', 1),\n",
       " ('such', 2),\n",
       " ('applications', 1),\n",
       " ('may', 1),\n",
       " ('be', 3),\n",
       " ('reduced', 1),\n",
       " ('several', 1),\n",
       " ('orders', 1),\n",
       " ('magnitude', 1),\n",
       " ('compared', 1),\n",
       " ('Hadoop', 3),\n",
       " ('implementation.', 1),\n",
       " ('Among', 1),\n",
       " ('class', 1),\n",
       " ('algorithms', 2),\n",
       " ('are', 1),\n",
       " ('training', 1),\n",
       " ('machine', 3),\n",
       " ('learning', 1),\n",
       " ('systems,', 1),\n",
       " ('formed', 1),\n",
       " ('initial', 1),\n",
       " ('impetus', 1),\n",
       " ('developing', 1),\n",
       " ('Spark.', 1),\n",
       " ('requires', 1),\n",
       " ('manager', 1),\n",
       " ('storage', 2),\n",
       " ('system.', 1),\n",
       " ('For', 2),\n",
       " ('management,', 1),\n",
       " ('supports', 2),\n",
       " ('standalone', 1),\n",
       " ('(native', 1),\n",
       " ('cluster,', 1),\n",
       " ('where', 2),\n",
       " ('you', 1),\n",
       " ('can', 4),\n",
       " ('launch', 2),\n",
       " ('either', 1),\n",
       " ('manually', 1),\n",
       " ('or', 4),\n",
       " ('scripts', 1),\n",
       " ('provided', 1),\n",
       " ('install', 1),\n",
       " ('package.', 1),\n",
       " ('It', 1),\n",
       " ('also', 2),\n",
       " ('possible', 1),\n",
       " ('run', 2),\n",
       " ('these', 1),\n",
       " ('daemons', 1),\n",
       " ('single', 2),\n",
       " ('testing),', 1),\n",
       " ('YARN,', 1),\n",
       " ('Mesos', 1),\n",
       " ('Kubernetes.', 1),\n",
       " ('storage,', 1),\n",
       " ('with', 2),\n",
       " ('wide', 1),\n",
       " ('variety,', 1),\n",
       " ('including', 1),\n",
       " ('Alluxio,', 1),\n",
       " ('Distributed', 1),\n",
       " ('File', 2),\n",
       " ('System', 2),\n",
       " ('(HDFS),', 1),\n",
       " ('MapR', 1),\n",
       " ('(MapR-FS),', 1),\n",
       " ('Cassandra,[14]', 1),\n",
       " ('OpenStack', 1),\n",
       " ('Swift,', 1),\n",
       " ('Amazon', 1),\n",
       " ('S3,', 1),\n",
       " ('Kudu,', 1),\n",
       " ('Lustre', 1),\n",
       " ('file', 2),\n",
       " ('system,', 1),\n",
       " ('custom', 1),\n",
       " ('solution', 1),\n",
       " ('implemented.', 1),\n",
       " ('pseudo-distributed', 1),\n",
       " ('local', 2),\n",
       " ('mode,', 1),\n",
       " ('usually', 1),\n",
       " ('used', 2),\n",
       " ('only', 1),\n",
       " ('development', 1),\n",
       " ('testing', 1),\n",
       " ('purposes,', 1),\n",
       " ('required', 1),\n",
       " ('system', 1),\n",
       " ('instead;', 1),\n",
       " ('scenario,', 1),\n",
       " ('one', 1),\n",
       " ('executor', 1),\n",
       " ('per', 1),\n",
       " ('CPU', 1),\n",
       " ('core.', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14951f56-ea65-4aa5-a89e-e94253fde1c7",
   "metadata": {},
   "source": [
    "\n",
    "determine what is the most frequent word in the poem, and how many times was it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd259897-6ba2-4670-99e1-c409b7ea2146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23), ('a', 18)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SortedWordCounts = sorted(wordCounts.collect(), key=lambda x:x[1], reverse=True)\n",
    "SortedWordCounts[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7d9bf44-3778-48c6-b848-d279395a350c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 23)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.reduce(lambda a, b: a if a[1] > b[1] else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21102744-5741-41a6-a0f6-6785c0c76fd9",
   "metadata": {},
   "source": [
    "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. \n",
    "\n",
    "One of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.\n",
    "\n",
    "You can mark an RDD to be persisted using the persist() or cache() methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d421ef-918f-4a8d-ac74-c3c590764ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(LinesWithSpark.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8e9536a-9bc7-4aba-94ab-96d3d5e80920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count():\n",
    "    return LinesWithSpark.count()\n",
    "def t():\n",
    "    %timeit count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "104341b2-5fe5-4cb8-a348-963fc2efc987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 ms ± 27.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d89e86d-b5d4-473c-94cb-c61bfaac272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.2 ms ± 2.97 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LinesWithSpark.cache()\n",
    "print(t())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
